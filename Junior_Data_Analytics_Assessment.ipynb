{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Han529/Fund-Scraper-Assessment/blob/main/Junior_Data_Analytics_Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13F Filings Scraper with Post-Fetch Caps (100 Managers / 3,284 Quarter Links)  \n",
        "**Note:** Applied AFTER getting all links.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3Mbf0JxLa64w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#                            Imports and Setup\n",
        "# ==============================================================================\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "from urllib.parse import urljoin\n",
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Optional, Tuple, Any, Set\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed # For Parallelism\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, retry_if_exception # For Backoff\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)-8s - %(threadName)-10s - %(message)s', # Added threadName\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "# Suppress overly verbose logs from underlying libraries\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
        "\n",
        "# --- Constants ---\n",
        "BASE_URL: str = \"https://13f.info\"\n",
        "DEFAULT_USER_AGENT: str = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "CSV_FILENAME: str = \"Fund Manager Shares Analysis.csv\"\n",
        "\n",
        "# Define the exact columns desired in the final output, in order\n",
        "TARGET_FINAL_COLUMNS: List[str] = [\n",
        "    'fund_name', 'filing_date', 'quarter', 'stock symbol', 'cl',\n",
        "    'value_usd_000', 'shares', 'change', 'pct_change', 'inferred_transaction_type'\n",
        "]\n",
        "\n",
        "# Parallelism Configuration\n",
        "MAX_WORKERS: int = 15 # Adjust based on your system/network and risk tolerance (start lower, e.g., 10)\n",
        "# Backoff Configuration for Tenacity\n",
        "MAX_RETRIES: int = 4 # Max attempts per request (1 initial + 3 retries)\n",
        "INITIAL_BACKOFF_DELAY: float = 1.0 # Seconds for first retry delay base\n",
        "MAX_BACKOFF_DELAY: float = 10.0 # Max seconds to wait between retries\n",
        "\n",
        "logging.info(\"Script started. Libraries imported, logging and constants configured.\")\n",
        "logging.info(f\"Parallelism enabled with MAX_WORKERS={MAX_WORKERS}\")\n",
        "logging.info(f\"Exponential backoff configured: MAX_RETRIES={MAX_RETRIES}, INITIAL_DELAY={INITIAL_BACKOFF_DELAY}s, MAX_DELAY={MAX_BACKOFF_DELAY}s\")\n",
        "\n",
        "# ==============================================================================\n",
        "#              Request Function with Exponential Backoff (using Tenacity)\n",
        "# ==============================================================================\n",
        "\n",
        "# Define what network/connection exceptions tenacity should retry on\n",
        "RETRYABLE_NETWORK_EXCEPTIONS = (\n",
        "    requests.exceptions.Timeout,\n",
        "    requests.exceptions.ConnectionError,\n",
        "    requests.exceptions.ChunkedEncodingError # Can happen on intermittent network issues\n",
        ")\n",
        "\n",
        "# Define specific HTTP status codes to retry on (server errors and rate limiting)\n",
        "RETRYABLE_STATUS_CODES = {429, 500, 502, 503, 504}\n",
        "\n",
        "# Custom retry condition check for HTTP errors\n",
        "def _should_retry_http_error(exception: BaseException) -> bool:\n",
        "    \"\"\"Return True if the exception is an HTTPError with a retryable status code.\"\"\"\n",
        "    return (\n",
        "        isinstance(exception, requests.exceptions.HTTPError) and\n",
        "        hasattr(exception, 'response') and # Ensure response attribute exists\n",
        "        exception.response is not None and\n",
        "        exception.response.status_code in RETRYABLE_STATUS_CODES\n",
        "    )\n",
        "\n",
        "# Define the retry decorator\n",
        "retry_config = retry(\n",
        "    stop=stop_after_attempt(MAX_RETRIES),\n",
        "    wait=wait_exponential(multiplier=1, min=INITIAL_BACKOFF_DELAY, max=MAX_BACKOFF_DELAY),\n",
        "    # Retry on specific network exceptions OR specific HTTP error codes\n",
        "    retry=(retry_if_exception_type(RETRYABLE_NETWORK_EXCEPTIONS) | retry_if_exception(_should_retry_http_error)),\n",
        "    # Log before sleeping on retry\n",
        "    before_sleep=lambda retry_state: logging.warning(\n",
        "        f\"Retrying request for {retry_state.args[0]} \"\n",
        "        f\"due to {retry_state.outcome.exception().__class__.__name__}: {retry_state.outcome.exception()} \"\n",
        "        f\"(Attempt {retry_state.attempt_number}/{MAX_RETRIES}). Waiting {retry_state.next_action.sleep:.2f}s...\"\n",
        "    )\n",
        ")\n",
        "\n",
        "@retry_config\n",
        "def make_request_with_retries(url: str, headers: Dict[str, str], params: Optional[Dict[str, Any]] = None, timeout: int = 45) -> requests.Response:\n",
        "    \"\"\"\n",
        "    Makes a GET request using requests library with exponential backoff for specific errors,\n",
        "    leveraging the tenacity library.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to request.\n",
        "        headers (Dict[str, str]): Request headers.\n",
        "        params (Optional[Dict[str, Any]]): URL parameters.\n",
        "        timeout (int): Request timeout in seconds.\n",
        "\n",
        "    Returns:\n",
        "        requests.Response: The response object if successful within retry limits.\n",
        "\n",
        "    Raises:\n",
        "        requests.exceptions.RequestException: If the request fails after all retries (or for non-retryable HTTP errors).\n",
        "        Exception: For other unexpected errors during the request phase.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, params=params, timeout=timeout)\n",
        "        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        return response # Success!\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        # Log the final non-retryable HTTP error before raising\n",
        "        if not _should_retry_http_error(http_err):\n",
        "            logging.error(f\"Request to {url} failed permanently with status {http_err.response.status_code}. Error: {http_err}\")\n",
        "        raise # Re-raise the exception (tenacity decides based on should_retry)\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        # Log other request exceptions before raising/retrying\n",
        "        logging.error(f\"Request to {url} failed with connection/timeout error: {req_err}\")\n",
        "        raise # Re-raise the exception for tenacity\n",
        "    except Exception as e:\n",
        "         logging.error(f\"Unexpected error during request to {url}: {e}\", exc_info=True)\n",
        "         raise # Re-raise unexpected errors\n",
        "\n",
        "# ==============================================================================\n",
        "#                        Scraping Function Definitions\n",
        "# ==============================================================================\n",
        "# --- Helper Functions ---\n",
        "def _clean_header(header_text: str) -> str:\n",
        "    \"\"\"Helper function to consistently clean table header text.\"\"\"\n",
        "    if not isinstance(header_text, str): return \"\"\n",
        "    text = header_text.lower().strip()\n",
        "    text = text.replace(' ', '_')\n",
        "    text = text.replace('($000)', 'usd_000')\n",
        "    text = text.replace('%', 'pct')\n",
        "    text = re.sub(r'[^\\w_]+', '', text) # Keep underscores\n",
        "    return text\n",
        "\n",
        "def _extract_headers(table_tag: BeautifulSoup) -> List[str]:\n",
        "    \"\"\"Robustly extracts and cleans header text from a table tag.\"\"\"\n",
        "    header_cells = []\n",
        "    thead = table_tag.find('thead')\n",
        "    if thead:\n",
        "        header_row = thead.find('tr')\n",
        "        if header_row: header_cells = header_row.find_all(['th', 'td'])\n",
        "        if not header_cells: header_cells = thead.find_all('th')\n",
        "    if not header_cells:\n",
        "        first_row = table_tag.find('tr')\n",
        "        if first_row: header_cells = first_row.find_all(['th', 'td'])\n",
        "    if not header_cells:\n",
        "        logging.warning(\"Could not find any header cells (th/td) using multiple strategies.\")\n",
        "        return []\n",
        "    headers = [_clean_header(h.get_text()) for h in header_cells]\n",
        "    headers = [h for h in headers if h] # Filter empty strings\n",
        "    # logging.info(f\"Extracted {len(headers)} headers: {headers}\") # Can be noisy\n",
        "    return headers\n",
        "\n",
        "# --- Fallback BS4 Parsing ---\n",
        "def parse_tables_directly_bs4(soup_or_table_tag: Any, url: str, single_table: bool = False) -> List[pd.DataFrame]:\n",
        "    \"\"\"Parses table(s) directly from HTML using BeautifulSoup. Used as a fallback.\"\"\"\n",
        "    dataframes = []\n",
        "    potential_tables = []\n",
        "    if single_table:\n",
        "        potential_tables = [soup_or_table_tag] if soup_or_table_tag else []\n",
        "        mode = \"single table\"\n",
        "    else:\n",
        "        potential_tables = soup_or_table_tag.find_all('table', {'class': re.compile(r'\\btable\\b')}) if soup_or_table_tag else []\n",
        "        mode = \"all tables\"\n",
        "    # logging.info(f\"[Direct BS4 - {mode}] Parsing {len(potential_tables)} potential table(s) on {url}\")\n",
        "    if not potential_tables: return []\n",
        "\n",
        "    for i, table_tag in enumerate(potential_tables):\n",
        "        # logging.debug(f\"[Direct BS4] Processing table {i+1}\")\n",
        "        headers_list = _extract_headers(table_tag)\n",
        "        if not headers_list: continue\n",
        "        tbody = table_tag.find('tbody')\n",
        "        rows_in_body = tbody.find_all('tr') if tbody else (table_tag.find_all('tr')[1:] if len(table_tag.find_all('tr')) > 1 else [])\n",
        "        if not rows_in_body: continue\n",
        "\n",
        "        data_rows = []\n",
        "        expected_cols = len(headers_list)\n",
        "        column_mismatch_logged = False\n",
        "        for row_idx, row in enumerate(rows_in_body):\n",
        "            cells = row.find_all('td')\n",
        "            actual_cols = len(cells)\n",
        "            if actual_cols == expected_cols:\n",
        "                row_data = [c.get_text(strip=True) if c else None for c in cells]\n",
        "                data_rows.append(row_data)\n",
        "            else:\n",
        "                 if not column_mismatch_logged:\n",
        "                    logging.warning(f\"[Direct BS4] Table {i+1} on {url}: Row {row_idx} has {actual_cols} cells, expected {expected_cols}. Skipping mismatched rows for this table.\")\n",
        "                    column_mismatch_logged = True\n",
        "        if data_rows:\n",
        "            try:\n",
        "                df = pd.DataFrame(data_rows, columns=headers_list)\n",
        "                df['SourceURL'] = url\n",
        "                df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "                dataframes.append(df)\n",
        "                # logging.info(f\"[Direct BS4] Successfully created DataFrame (Shape: {df.shape}) from table {i+1} on {url}\")\n",
        "            except Exception as e:\n",
        "                 logging.error(f\"[Direct BS4] Error creating DataFrame for table {i+1} on {url}: {e}\", exc_info=True)\n",
        "    return dataframes\n",
        "\n",
        "# --- Primary Scraping Functions (using make_request_with_retries) ---\n",
        "\n",
        "def scrape_az_index_links(managers_index_page_url: str) -> List[str]:\n",
        "    \"\"\"Scrapes the A-Z (and 0-9) index links from the main managers listing page.\"\"\"\n",
        "    logging.info(f\"Scraping A-Z index links from: {managers_index_page_url}\")\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT}\n",
        "    index_links = set()\n",
        "    try:\n",
        "        response = make_request_with_retries(managers_index_page_url, headers=headers, timeout=30)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        index_container = soup.find('div', class_='mb-12')\n",
        "        links_in_container = index_container.find_all('a', href=True) if index_container else soup.select('ul > li > a[href]') # Fallback selector\n",
        "\n",
        "        for link_tag in links_in_container:\n",
        "            href = link_tag.get('href')\n",
        "            if isinstance(href, str) and href.startswith('/managers/') and len(href.split('/')[-1]) == 1:\n",
        "                index_links.add(href)\n",
        "\n",
        "        if not index_links:\n",
        "             logging.warning(f\"No valid A-Z index links extracted from {managers_index_page_url}\")\n",
        "             return []\n",
        "\n",
        "        logging.info(f\"Found {len(index_links)} A-Z index links: {sorted(list(index_links))}\")\n",
        "        return sorted(list(index_links))\n",
        "\n",
        "    except Exception as e: # Catch errors after retries from make_request or parsing errors\n",
        "        logging.error(f\"Failed to scrape or parse A-Z index page {managers_index_page_url} after retries. Error: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def scrape_managers_from_letter_page(letter_page_relative_url: str, base_url: str) -> Set[str]:\n",
        "    \"\"\"Scrapes all actual manager links from a specific A-Z index page.\"\"\"\n",
        "    full_url = urljoin(base_url, letter_page_relative_url)\n",
        "    logging.info(f\"Scraping manager links from letter page: {full_url}\")\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT}\n",
        "    manager_links_on_page = set()\n",
        "    try:\n",
        "        response = make_request_with_retries(full_url, headers=headers, timeout=45)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        all_links_on_page = soup.find_all('a', href=True)\n",
        "        for link_tag in all_links_on_page:\n",
        "            href = link_tag.get('href')\n",
        "            if isinstance(href, str) and href.startswith('/manager/'):\n",
        "                path_parts = href.strip('/').split('/')\n",
        "                if len(path_parts) == 2 and path_parts[0] == 'manager' and len(path_parts[1]) > 1:\n",
        "                    manager_links_on_page.add(href)\n",
        "        # logging.info(f\"Found {len(manager_links_on_page)} manager links on {full_url}\") # Can be noisy\n",
        "        return manager_links_on_page\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to scrape or parse letter page {full_url} after retries. Error: {e}\")\n",
        "        return set()\n",
        "\n",
        "\n",
        "def get_all_manager_links_via_az_index(base_url: str, executor: ThreadPoolExecutor) -> Tuple[List[str], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Orchestrates scraping manager links from A-Z index pages in parallel.\n",
        "    \"\"\"\n",
        "    all_unique_manager_links: Set[str] = set()\n",
        "    manager_counts_per_index_page: Dict[str, int] = {}\n",
        "    managers_main_page_url = urljoin(base_url, \"/managers\")\n",
        "\n",
        "    logging.info(\"Starting A-Z index link scraping...\")\n",
        "    az_index_links = scrape_az_index_links(managers_main_page_url)\n",
        "    if not az_index_links:\n",
        "        logging.critical(\"Could not retrieve A-Z index links. Cannot proceed.\")\n",
        "        return [], {}\n",
        "\n",
        "    total_az_pages = len(az_index_links)\n",
        "    logging.info(f\"Found {total_az_pages} A-Z index pages to process in parallel.\")\n",
        "    print(f\"Found {total_az_pages} A-Z index pages. Processing in parallel...\")\n",
        "\n",
        "    # Submit tasks to the executor\n",
        "    future_to_url = {executor.submit(scrape_managers_from_letter_page, url, base_url): url for url in az_index_links}\n",
        "    processed_count = 0\n",
        "\n",
        "    for future in as_completed(future_to_url):\n",
        "        rel_url = future_to_url[future]\n",
        "        index_char = rel_url.split('/')[-1].upper()\n",
        "        processed_count += 1\n",
        "        try:\n",
        "            manager_links_set = future.result() # Get the set of links from the completed task\n",
        "            count = len(manager_links_set)\n",
        "            manager_counts_per_index_page[rel_url] = count\n",
        "            all_unique_manager_links.update(manager_links_set)\n",
        "            logging.info(f\"[{processed_count}/{total_az_pages}] Index '{index_char}' completed. Found {count} links. Total unique: {len(all_unique_manager_links)}\")\n",
        "            print(f\"  Processed index '{index_char}'. Found {count} links. Total unique: {len(all_unique_manager_links)}\")\n",
        "        except Exception as exc:\n",
        "            logging.error(f\"Index page {rel_url} generated an exception: {exc}\")\n",
        "            manager_counts_per_index_page[rel_url] = 0 # Record failure\n",
        "\n",
        "    final_unique_count = len(all_unique_manager_links)\n",
        "    logging.info(f\"Finished parallel A-Z index scraping. Total unique manager links found: {final_unique_count}\")\n",
        "    print(f\"\\nFinished A-Z index scraping. Found {final_unique_count} unique manager links.\")\n",
        "    return sorted(list(all_unique_manager_links)), manager_counts_per_index_page\n",
        "\n",
        "\n",
        "def scrape_quarter_links_task(rel_link: str, base_url: str) -> Tuple[str, List[str]]:\n",
        "    \"\"\"Task wrapper for scraping quarter links for use with executor.\"\"\"\n",
        "    links = scrape_quarter_links_from_manager(rel_link, base_url) # Uses make_request internally\n",
        "    return rel_link, links\n",
        "\n",
        "def scrape_quarter_links_from_manager(relative_manager_link: str, base_url: str) -> List[str]:\n",
        "    \"\"\"Scrapes quarterly filing URLs from a manager page.\"\"\"\n",
        "    full_url = urljoin(base_url, relative_manager_link)\n",
        "    # logging.info(f\"Scraping quarter links from: {full_url}\") # Can be noisy in parallel\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT}\n",
        "    quarter_links = []\n",
        "    try:\n",
        "        response = make_request_with_retries(full_url, headers=headers, timeout=30)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        table = soup.find('table', {'class': re.compile(r'\\btable\\b')})\n",
        "        if not table: return []\n",
        "        header_row = table.find('thead') or table.find('tr')\n",
        "        if not header_row: return []\n",
        "        headers_list = header_row.find_all(['th', 'td'])\n",
        "        quarter_col_index = -1\n",
        "        for i, header in enumerate(headers_list):\n",
        "            if header.get_text(strip=True).lower() == \"quarter\":\n",
        "                quarter_col_index = i; break\n",
        "        if quarter_col_index == -1: return []\n",
        "        tbody = table.find('tbody')\n",
        "        data_rows = tbody.find_all('tr') if tbody else (table.find_all('tr')[1:] if len(table.find_all('tr')) > 1 else [])\n",
        "        if not data_rows: return []\n",
        "        for row in data_rows:\n",
        "            cells = row.find_all('td')\n",
        "            if len(cells) > quarter_col_index:\n",
        "                link_tag = cells[quarter_col_index].find('a', href=True)\n",
        "                href_val = link_tag.get('href') if link_tag else None\n",
        "                if isinstance(href_val, str) and href_val.strip():\n",
        "                    quarter_links.append(urljoin(base_url, href_val))\n",
        "        # logging.info(f\"Found {len(quarter_links)} quarter links on {full_url}.\") # Noisy\n",
        "        return quarter_links\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed scrape_quarter_links_from_manager for {full_url}. Error: {e}\")\n",
        "        return [] # Return empty list on failure after retries\n",
        "\n",
        "# ---\n",
        "def scrape_fund_name(manager_page_url: str) -> str:\n",
        "    \"\"\"Scrapes the fund name from the manager's main page (using retries).\"\"\"\n",
        "    # logging.info(f\"Scraping fund name from: {manager_page_url}\") # Noisy\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT}\n",
        "    default_name = \"Unknown Fund Name\"\n",
        "    try:\n",
        "        response = make_request_with_retries(manager_page_url, headers=headers, timeout=25)\n",
        "        soup = BeautifulSoup(response.text, 'lxml')\n",
        "        h1_tag = soup.find('h1')\n",
        "        if h1_tag:\n",
        "            name = h1_tag.get_text(strip=True)\n",
        "            if ' CIK#' in name: name = name.split(' CIK#')[0].strip()\n",
        "            # logging.info(f\"Found fund name via H1: {name}\") # Noisy\n",
        "            return name if name else default_name\n",
        "        else: # Fallback to URL parsing\n",
        "            logging.warning(f\"H1 tag not found on {manager_page_url}. Attempting URL parsing.\")\n",
        "            try:\n",
        "                path_parts = manager_page_url.strip('/').split('/')\n",
        "                if len(path_parts) > 3 and path_parts[-2] == 'manager':\n",
        "                    url_name_part = path_parts[-1]\n",
        "                    first_dash_index = url_name_part.find('-')\n",
        "                    if first_dash_index != -1:\n",
        "                         parsed_name = url_name_part[first_dash_index:].strip('-').replace('-', ' ').title()\n",
        "                         if parsed_name: return parsed_name\n",
        "            except Exception: pass\n",
        "            return default_name\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed scrape_fund_name for {manager_page_url} after retries. Error: {e}\")\n",
        "        return default_name\n",
        "\n",
        "# ---\n",
        "def scrape_table_data_and_metadata(url: str) -> Dict[str, Any]:\n",
        "    \"\"\"Scrapes table data and metadata from a filing URL (using retries).\"\"\"\n",
        "    # logging.info(f\"Scraping table data & metadata from: {url}\") # Noisy\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT, 'Accept': 'application/json, text/javascript, */*; q=0.01', 'X-Requested-With': 'XMLHttpRequest'}\n",
        "    result: Dict[str, Any] = {'dataframes': [], 'filing_date': None, 'quarter': None}\n",
        "    html_content = None\n",
        "    soup = None\n",
        "    try:\n",
        "        response_page = make_request_with_retries(url, headers=headers, timeout=45)\n",
        "        html_content = response_page.text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to retrieve page {url} after retries. Error: {e}\")\n",
        "        return result # Cannot proceed\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "        # Extract Metadata\n",
        "        date_dt = soup.find('dt', string=lambda t: t and 'Date filed' in t.strip())\n",
        "        if date_dt and date_dt.find_next_sibling('dd'): result['filing_date'] = date_dt.find_next_sibling('dd').get_text(strip=True)\n",
        "        qtr_h = soup.find(['h1', 'h2', 'h3'], string=lambda t: t and ('13F Holdings' in t or 'Quarter' in t))\n",
        "        if qtr_h: result['quarter'] = qtr_h.get_text(strip=True)\n",
        "    except Exception as e: logging.error(f\"Error parsing metadata from {url}: {e}\")\n",
        "\n",
        "    target_table = soup.find('table', id='filingAggregated') if soup else None\n",
        "    if not target_table:\n",
        "        # logging.warning(f\"Target table not found. Parsing all tables on page: {url}\")\n",
        "        if soup: result['dataframes'] = parse_tables_directly_bs4(soup, url)\n",
        "        return result\n",
        "\n",
        "    headers_list = _extract_headers(target_table)\n",
        "    if not headers_list:\n",
        "        logging.error(f\"CRITICAL: Failed to extract HTML headers from target table on {url}.\")\n",
        "        result['dataframes'] = parse_tables_directly_bs4(target_table, url, single_table=True)\n",
        "        return result\n",
        "\n",
        "    data_url_path = target_table.get('data-url')\n",
        "    ajax_succeeded = False\n",
        "    if data_url_path:\n",
        "        ajax_url = urljoin(BASE_URL, data_url_path)\n",
        "        # logging.info(f\"AJAX endpoint detected. Attempting fetch from: {ajax_url}\") # Noisy\n",
        "        try:\n",
        "            response_ajax = make_request_with_retries(ajax_url, headers=headers, timeout=45)\n",
        "            ajax_data = response_ajax.json()\n",
        "            table_rows_data = ajax_data.get('data')\n",
        "            if table_rows_data and isinstance(table_rows_data, list):\n",
        "                expected_cols = len(headers_list)\n",
        "                cleaned_rows = []\n",
        "                for row in table_rows_data:\n",
        "                    if isinstance(row, list):\n",
        "                        actual_cols = len(row)\n",
        "                        if actual_cols >= expected_cols:\n",
        "                            processed_row = row[:expected_cols]\n",
        "                            cleaned_cells = [BeautifulSoup(str(c), 'lxml').get_text(strip=True) if isinstance(c, str) else c for c in processed_row]\n",
        "                            cleaned_rows.append(cleaned_cells)\n",
        "                if cleaned_rows:\n",
        "                    df = pd.DataFrame(cleaned_rows, columns=headers_list)\n",
        "                    df['SourceURL'] = url\n",
        "                    result['dataframes'].append(df)\n",
        "                    ajax_succeeded = True\n",
        "                    # logging.info(f\"Created DataFrame (Shape: {df.shape}) from AJAX.\") # Noisy\n",
        "            else: logging.warning(f\"AJAX 'data' key missing or not list for {ajax_url}.\")\n",
        "        except Exception as e: logging.error(f\"AJAX request/processing failed for {ajax_url} after retries. Error: {e}\")\n",
        "        if ajax_succeeded: return result\n",
        "        else: logging.warning(f\"AJAX did not yield DataFrame for {url}.\")\n",
        "\n",
        "    # Fallback if no data-url or AJAX failed\n",
        "    # logging.info(f\"Falling back to direct HTML parsing for target table on: {url}\") # Noisy\n",
        "    result['dataframes'] = parse_tables_directly_bs4(target_table, url, single_table=True)\n",
        "    return result\n",
        "\n",
        "\n",
        "# --- Wrapper function for parallel data scraping task ---\n",
        "def scrape_filing_data_task(data_url: str, fund_name: str) -> List[pd.DataFrame]:\n",
        "    \"\"\"Scrapes data for one filing URL and adds fund_name.\"\"\"\n",
        "    scrape_result = scrape_table_data_and_metadata(data_url)\n",
        "    processed_dfs = []\n",
        "    for df in scrape_result.get('dataframes', []):\n",
        "        df['fund_name'] = fund_name\n",
        "        df['filing_date'] = scrape_result.get('filing_date')\n",
        "        df['quarter'] = scrape_result.get('quarter')\n",
        "        processed_dfs.append(df)\n",
        "    return processed_dfs\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#                        Execution Stage 1: Get All Manager Links (A-Z Parallel)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 1: Scraping All Manager Links via A-Z Index (Parallel)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time_stage1 = time.time()\n",
        "all_manager_links = []\n",
        "manager_counts = {}\n",
        "\n",
        "# Use ThreadPoolExecutor for parallel execution\n",
        "# The 'with' statement ensures threads are cleaned up properly\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS, thread_name_prefix='AZScrape') as executor:\n",
        "    all_manager_links, manager_counts = get_all_manager_links_via_az_index(BASE_URL, executor)\n",
        "\n",
        "end_time_stage1 = time.time()\n",
        "print(f\"\\n--- Manager Link Scraping Summary ---\")\n",
        "print(f\"Total unique manager links found via A-Z Index: {len(all_manager_links)}\")\n",
        "if manager_counts:\n",
        "    print(\"\\nCounts per Index Page:\")\n",
        "    # Sort counts by index page URL for consistent output\n",
        "    for page_url, count in sorted(manager_counts.items()):\n",
        "        index_char = page_url.split('/')[-1].upper()\n",
        "        print(f\"  Index '{index_char}' ({page_url}): {count} managers\")\n",
        "print(f\"Stage 1 Duration: {end_time_stage1 - start_time_stage1:.2f} seconds\")\n",
        "\n",
        "if not all_manager_links:\n",
        "    print(\"\\nCRITICAL: No manager links were found. Cannot proceed.\")\n",
        "    exit()\n",
        "\n",
        "# Optional: Limit managers for testing (apply AFTER getting all links)\n",
        "test_limit_stage1 = 100 # Set a reasonable limit for testing\n",
        "if len(all_manager_links) > test_limit_stage1:\n",
        "    logging.info(f\"TESTING: Limiting run to first {test_limit_stage1} managers found.\")\n",
        "    print(f\"\\n*** LIMITING MANAGERS TO {test_limit_stage1} FOR TESTING/ASSESSMENT ***\\n\")\n",
        "    manager_links_relative = all_manager_links[:test_limit_stage1]\n",
        "else:\n",
        "     manager_links_relative = all_manager_links # Process all if less than limit\n",
        "\n",
        "time.sleep(random.uniform(0.5, 1.0)) # Small delay\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#                 Execution Stage 2: Get Quarter Links (Parallel)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 2: Scraping Quarter Links for Each Selected Manager (Parallel)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time_stage2 = time.time()\n",
        "manager_quarter_links: Dict[str, List[str]] = {}\n",
        "num_managers_to_process = len(manager_links_relative)\n",
        "processed_count_stage2 = 0\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS, thread_name_prefix='QuarterLinkScrape') as executor:\n",
        "    future_to_rel_link = {executor.submit(scrape_quarter_links_task, rel_link, BASE_URL): rel_link for rel_link in manager_links_relative}\n",
        "\n",
        "    for future in as_completed(future_to_rel_link):\n",
        "        rel_link = future_to_rel_link[future]\n",
        "        processed_count_stage2 += 1\n",
        "        try:\n",
        "            _rel_link_result, links = future.result() # Unpack tuple\n",
        "            manager_quarter_links[rel_link] = links\n",
        "            # Log progress periodically or based on count\n",
        "            if processed_count_stage2 % 50 == 0 or processed_count_stage2 == num_managers_to_process:\n",
        "                 logging.info(f\"Quarter links progress: {processed_count_stage2}/{num_managers_to_process} managers processed.\")\n",
        "                 print(f\"  Processed quarter links for manager {processed_count_stage2}/{num_managers_to_process}...\")\n",
        "            if not links:\n",
        "                 logging.warning(f\"No quarter links found or error occurred for {rel_link}\")\n",
        "        except Exception as exc:\n",
        "            logging.error(f\"Getting quarter links for {rel_link} generated an exception: {exc}\")\n",
        "            manager_quarter_links[rel_link] = [] # Ensure entry exists even on failure\n",
        "\n",
        "end_time_stage2 = time.time()\n",
        "print(f\"\\nFinished getting quarter links stage for {len(manager_quarter_links)} managers.\")\n",
        "print(f\"Stage 2 Duration: {end_time_stage2 - start_time_stage2:.2f} seconds\")\n",
        "\n",
        "# ==============================================================================\n",
        "#                 Execution Stage 3: Scrape Filing Data (Parallel)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 3: Scraping Filing Data for Each Manager/Quarter (Parallel)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time_stage3 = time.time()\n",
        "all_scraped_dataframes: List[pd.DataFrame] = []\n",
        "fund_name_map: Dict[str, str] = {} # Cache fund names\n",
        "\n",
        "# Prepare list of tasks: (data_url, fund_name)\n",
        "tasks_to_submit = []\n",
        "print(\"Preparing filing data scrape tasks...\")\n",
        "for i, (manager_link, quarter_links) in enumerate(manager_quarter_links.items()):\n",
        "    # Get or Cache Fund Name\n",
        "    if manager_link not in fund_name_map:\n",
        "         manager_page_url = urljoin(BASE_URL, manager_link)\n",
        "         fund_name_map[manager_link] = scrape_fund_name(manager_page_url)\n",
        "         # Brief pause after name scrape, before launching quarter scrapes\n",
        "         time.sleep(random.uniform(0.1, 0.4))\n",
        "\n",
        "    current_fund_name = fund_name_map[manager_link]\n",
        "\n",
        "    # Determine URLs to scrape for this manager\n",
        "    urls_to_scrape = quarter_links if quarter_links else [urljoin(BASE_URL, manager_link)]\n",
        "    if not quarter_links:\n",
        "        logging.warning(f\"No specific quarter links for {manager_link}, will attempt manager page scrape.\")\n",
        "\n",
        "    for data_url in urls_to_scrape:\n",
        "        tasks_to_submit.append((data_url, current_fund_name))\n",
        "\n",
        "total_filing_urls = len(tasks_to_submit)\n",
        "print(f\"Prepared {total_filing_urls} total filing URLs to scrape in parallel.\")\n",
        "logging.info(f\"Submitting {total_filing_urls} filing data scraping tasks to executor.\")\n",
        "\n",
        "processed_count_stage3 = 0\n",
        "# Use ThreadPoolExecutor for parallel data scraping\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS, thread_name_prefix='FilingDataScrape') as executor:\n",
        "    future_to_task_info = {executor.submit(scrape_filing_data_task, url, name): (url, name) for url, name in tasks_to_submit}\n",
        "\n",
        "    for future in as_completed(future_to_task_info):\n",
        "        url, name = future_to_task_info[future]\n",
        "        processed_count_stage3 += 1\n",
        "        try:\n",
        "            list_of_dfs = future.result() # Result is a list of DFs for that URL\n",
        "            if list_of_dfs:\n",
        "                all_scraped_dataframes.extend(list_of_dfs)\n",
        "                # Log progress periodically\n",
        "                if processed_count_stage3 % 100 == 0 or processed_count_stage3 == total_filing_urls:\n",
        "                    logging.info(f\"Filing data progress: {processed_count_stage3}/{total_filing_urls} URLs processed. Total DFs collected: {len(all_scraped_dataframes)}\")\n",
        "                    print(f\"  Processed filing URL {processed_count_stage3}/{total_filing_urls}...\")\n",
        "            # else: # No dataframes found for this URL (already logged in scrape function)\n",
        "                # pass\n",
        "        except Exception as exc:\n",
        "            logging.error(f\"Scraping task for URL {url} (Fund: {name}) generated an exception: {exc}\")\n",
        "\n",
        "end_time_stage3 = time.time()\n",
        "print(\"\\n--- Finished Scraping All Filing Data ---\")\n",
        "print(f\"Stage 3 Duration: {end_time_stage3 - start_time_stage3:.2f} seconds\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#               Execution Stage 4: Data Consolidation & Processing\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 4: Consolidating and Processing All Scraped Data\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time_stage4 = time.time()\n",
        "\n",
        "if not all_scraped_dataframes:\n",
        "    logging.warning(\"No dataframes were scraped in Stage 3. Cannot proceed with processing.\")\n",
        "    final_processed_df = pd.DataFrame() # Ensure variable exists but is empty\n",
        "else:\n",
        "    # --- Combine all scraped dataframes ---\n",
        "    logging.info(f\"Combining {len(all_scraped_dataframes)} scraped dataframes...\")\n",
        "    print(f\"Combining {len(all_scraped_dataframes)} scraped tables...\")\n",
        "    try:\n",
        "        combined_df = pd.concat(all_scraped_dataframes, ignore_index=True, sort=False)\n",
        "        logging.info(f\"Initial combined DataFrame shape: {combined_df.shape}\")\n",
        "        print(f\"Combined DataFrame shape: {combined_df.shape}.\")\n",
        "        # It's crucial to work on a copy for significant processing\n",
        "        processed_df = combined_df.copy()\n",
        "        del combined_df, all_scraped_dataframes # Free up memory\n",
        "    except Exception as e:\n",
        "        logging.critical(f\"CRITICAL Error during DataFrame concatenation: {e}\", exc_info=True)\n",
        "        processed_df = pd.DataFrame() # Assign empty df on critical error\n",
        "\n",
        "# --- Process the combined dataframe (only if concatenation was successful) ---\n",
        "if not processed_df.empty:\n",
        "    logging.info(\"Starting data processing steps...\")\n",
        "    print(\"\\nProcessing combined data...\")\n",
        "\n",
        "    # --- Standardize Column Names ---\n",
        "    processed_df.columns = [_clean_header(col) for col in processed_df.columns]\n",
        "    logging.info(f\"Standardized column names: {processed_df.columns.tolist()}\")\n",
        "\n",
        "    # --- Filter for Common Stock ('COM') ---\n",
        "    if 'cl' in processed_df.columns:\n",
        "        original_rows = len(processed_df)\n",
        "        processed_df = processed_df[processed_df['cl'].astype(str).str.upper() == 'COM']\n",
        "        logging.info(f\"Filtered for 'cl' == 'COM'. Rows reduced from {original_rows} to {len(processed_df)}.\")\n",
        "        print(f\"Filtered for 'COM' stock. Rows remaining: {len(processed_df)}.\")\n",
        "    else: logging.warning(\"'cl' column not found.\")\n",
        "\n",
        "    # --- Data Type Conversion and Cleaning ---\n",
        "    logging.info(\"Cleaning and converting data types...\")\n",
        "    # Shares\n",
        "    if 'shares' in processed_df.columns:\n",
        "        processed_df['shares'] = processed_df['shares'].astype(str).str.replace(',', '', regex=False)\n",
        "        processed_df['shares_numeric'] = pd.to_numeric(processed_df['shares'], errors='coerce')\n",
        "        if processed_df['shares_numeric'].isna().any(): logging.warning(\"Non-numeric shares coerced to NaN.\")\n",
        "    else: processed_df['shares_numeric'] = np.nan\n",
        "    # Value\n",
        "    value_col = 'value_usd_000'\n",
        "    if value_col in processed_df.columns:\n",
        "         processed_df[value_col] = processed_df[value_col].astype(str).str.replace(',', '', regex=False)\n",
        "         processed_df[value_col] = pd.to_numeric(processed_df[value_col], errors='coerce')\n",
        "         if processed_df[value_col].isna().any(): logging.warning(f\"Non-numeric '{value_col}' coerced to NaN.\")\n",
        "    else: logging.warning(f\"Value column '{value_col}' not found.\")\n",
        "    # Quarter Period\n",
        "    if 'quarter' in processed_df.columns:\n",
        "        def parse_quarter_to_period(q_str: Optional[str]) -> Optional[pd.Period]:\n",
        "            if not isinstance(q_str, str): return pd.NaT\n",
        "            match = re.search(r'(Q[1-4])\\s*(\\d{4})', q_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                q_num, year = match.group(1).upper(), match.group(2)\n",
        "                try: return pd.Period(f\"{year}{q_num}\", freq='Q')\n",
        "                except ValueError: return pd.NaT\n",
        "            return pd.NaT\n",
        "        processed_df['quarter_period'] = processed_df['quarter'].apply(parse_quarter_to_period)\n",
        "        if processed_df['quarter_period'].isna().any(): logging.warning(\"Some quarters could not be parsed.\")\n",
        "    else: processed_df['quarter_period'] = pd.NaT\n",
        "\n",
        "    # --- Identify Stock Identifier ---\n",
        "    stock_id_col = 'sym' if 'sym' in processed_df.columns else ('stock_symbol' if 'stock_symbol' in processed_df.columns else None)\n",
        "    if not stock_id_col: logging.error(\"CRITICAL: No stock identifier found.\")\n",
        "\n",
        "    # --- Calculate Changes ---\n",
        "    can_calculate_changes = stock_id_col and ('shares_numeric' in processed_df.columns) and ('quarter_period' in processed_df.columns) and processed_df['quarter_period'].notna().any()\n",
        "    if can_calculate_changes:\n",
        "        logging.info(f\"Calculating changes grouped by 'fund_name', '{stock_id_col}'...\")\n",
        "        print(f\"Calculating quarterly changes grouped by fund and '{stock_id_col}'...\")\n",
        "        essential_cols = ['fund_name', stock_id_col, 'quarter_period', 'shares_numeric']\n",
        "        processed_df.dropna(subset=[col for col in essential_cols if col in processed_df.columns], inplace=True)\n",
        "        processed_df = processed_df.sort_values(by=['fund_name', stock_id_col, 'quarter_period'], ascending=True)\n",
        "        group_cols = ['fund_name', stock_id_col]\n",
        "        for col in ['change', 'pct_change', 'inferred_transaction_type']:\n",
        "             if col not in processed_df.columns: processed_df[col] = pd.NA\n",
        "        processed_df['change'] = processed_df.groupby(group_cols, observed=True)['shares_numeric'].diff().fillna(0)\n",
        "        pct_change_raw = processed_df.groupby(group_cols, observed=True)['shares_numeric'].pct_change()\n",
        "        processed_df['pct_change'] = pct_change_raw.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "        change_numeric = pd.to_numeric(processed_df['change'], errors='coerce')\n",
        "        conditions = [change_numeric < 0, change_numeric == 0, change_numeric > 0]\n",
        "        choices = ['Sell', 'Hold', 'Buy']\n",
        "        processed_df['inferred_transaction_type'] = np.select(conditions, choices, default='Unknown')\n",
        "        logging.info(\"Change calculations complete.\")\n",
        "        print(\"Quarterly change calculations complete.\")\n",
        "        try: processed_df['change'] = processed_df['change'].astype(pd.Int64Dtype())\n",
        "        except TypeError: pass\n",
        "    else:\n",
        "        logging.warning(\"Skipping quarterly change calculations.\")\n",
        "        print(\"Skipping quarterly change calculations.\")\n",
        "        for col in ['change', 'pct_change', 'inferred_transaction_type']:\n",
        "             if col not in processed_df.columns: processed_df[col] = pd.NA\n",
        "\n",
        "    # --- Final Column Formatting ---\n",
        "    logging.info(\"Formatting final columns...\")\n",
        "    print(\"\\nFormatting final columns for output...\")\n",
        "    if stock_id_col == 'sym' and 'sym' in processed_df.columns:\n",
        "        processed_df.rename(columns={'sym': 'stock symbol'}, inplace=True)\n",
        "        logging.info(\"Renamed 'sym' column to 'stock symbol'.\")\n",
        "    elif 'stock symbol' not in processed_df.columns and 'stock symbol' in TARGET_FINAL_COLUMNS:\n",
        "         logging.warning(\"Final target 'stock symbol' column not found. Creating with NaNs.\")\n",
        "         processed_df['stock symbol'] = np.nan\n",
        "\n",
        "    final_columns_existing = [col for col in TARGET_FINAL_COLUMNS if col in processed_df.columns]\n",
        "    missing_target_cols = [col for col in TARGET_FINAL_COLUMNS if col not in final_columns_existing]\n",
        "    if missing_target_cols: logging.warning(f\"Final target columns missing: {missing_target_cols}\")\n",
        "\n",
        "    final_processed_df = processed_df[final_columns_existing]\n",
        "    logging.info(f\"Final selected columns: {final_processed_df.columns.tolist()}\")\n",
        "    print(f\"Final DataFrame columns: {final_processed_df.columns.tolist()}\")\n",
        "    final_processed_df = final_processed_df.drop(columns=['shares_numeric', 'quarter_period'], errors='ignore')\n",
        "\n",
        "else: # combined_df was empty or processing failed\n",
        "    logging.warning(\"Combined DataFrame was empty or processing failed.\")\n",
        "    final_processed_df = pd.DataFrame()\n",
        "\n",
        "end_time_stage4 = time.time()\n",
        "print(f\"\\nStage 4 Duration: {end_time_stage4 - start_time_stage4:.2f} seconds\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#                        Execution Stage 5: Final Output\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 5: Final Output\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if not final_processed_df.empty:\n",
        "    # --- Print Head ---\n",
        "    print(f\"\\n--- First 5 rows of the final DataFrame (Shape: {final_processed_df.shape}) ---\")\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.width', 200)\n",
        "    pd.set_option('display.max_colwidth', 70)\n",
        "    print(final_processed_df.head().to_string())\n",
        "    pd.reset_option('display.max_colwidth')\n",
        "\n",
        "    # --- Save to CSV ---\n",
        "    print(f\"\\n--- Saving final DataFrame to '{CSV_FILENAME}' ---\")\n",
        "    try:\n",
        "        final_processed_df.to_csv(CSV_FILENAME, index=False, encoding='utf-8-sig')\n",
        "        print(f\"Successfully saved data ({len(final_processed_df)} rows) to '{CSV_FILENAME}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL ERROR: Failed to save the final DataFrame to CSV '{CSV_FILENAME}'. Error: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nFinal DataFrame is empty or was not created due to errors. No CSV file saved.\")\n",
        "\n",
        "# --- Calculate and Print Total Runtime ---\n",
        "total_end_time = time.time()\n",
        "# Need start time from the very beginning if defined, otherwise calculate from stages\n",
        "# Assuming start_time_stage1 exists\n",
        "if 'start_time_stage1' in locals():\n",
        "     total_runtime = total_end_time - start_time_stage1\n",
        "     print(f\"\\nTotal Script Runtime: {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)\")\n",
        "else:\n",
        "     # Sum stage durations if start_time_stage1 wasn't captured\n",
        "     total_runtime = (end_time_stage1 - start_time_stage1 if 'start_time_stage1' in locals() else 0) + \\\n",
        "                     (end_time_stage2 - start_time_stage2 if 'start_time_stage2' in locals() else 0) + \\\n",
        "                     (end_time_stage3 - start_time_stage3 if 'start_time_stage3' in locals() else 0) + \\\n",
        "                     (end_time_stage4 - start_time_stage4 if 'start_time_stage4' in locals() else 0)\n",
        "     if total_runtime > 0:\n",
        "          print(f\"\\nTotal Script Runtime (Sum of Stages): {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Script execution finished ---\")"
      ],
      "metadata": {
        "id": "0b-g9O0ldSji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3841e7b-9ec9-46e0-dd22-84b4b69a3086"
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            " Stage 1: Scraping All Manager Links via A-Z Index (Parallel)\n",
            "======================================================================\n",
            "Found 27 A-Z index pages. Processing in parallel...\n",
            "  Processed index '0'. Found 72 links. Total unique: 72\n",
            "  Processed index 'J'. Found 170 links. Total unique: 242\n",
            "  Processed index 'K'. Found 309 links. Total unique: 551\n",
            "  Processed index 'D'. Found 346 links. Total unique: 897\n",
            "  Processed index 'N'. Found 363 links. Total unique: 1260\n",
            "  Processed index 'Q'. Found 73 links. Total unique: 1333\n",
            "  Processed index 'I'. Found 315 links. Total unique: 1648\n",
            "  Processed index 'O'. Found 250 links. Total unique: 1898\n",
            "  Processed index 'G'. Found 484 links. Total unique: 2382\n",
            "  Processed index 'H'. Found 458 links. Total unique: 2840\n",
            "  Processed index 'L'. Found 444 links. Total unique: 3284\n",
            "  Processed index 'A'. Found 1000 links. Total unique: 4284\n",
            "  Processed index 'E'. Found 369 links. Total unique: 4653\n",
            "  Processed index 'F'. Found 579 links. Total unique: 5232\n",
            "  Processed index 'B'. Found 777 links. Total unique: 6009\n",
            "  Processed index 'U'. Found 97 links. Total unique: 6106\n",
            "  Processed index 'X'. Found 16 links. Total unique: 6122\n",
            "  Processed index 'C'. Found 1139 links. Total unique: 7261\n",
            "  Processed index 'Y'. Found 42 links. Total unique: 7303\n",
            "  Processed index 'Z'. Found 37 links. Total unique: 7340\n",
            "  Processed index 'M'. Found 735 links. Total unique: 8075\n",
            "  Processed index 'P'. Found 718 links. Total unique: 8793\n",
            "  Processed index 'V'. Found 254 links. Total unique: 9047\n",
            "  Processed index 'R'. Found 477 links. Total unique: 9524\n",
            "  Processed index 'T'. Found 561 links. Total unique: 10085\n",
            "  Processed index 'W'. Found 442 links. Total unique: 10527\n",
            "  Processed index 'S'. Found 1172 links. Total unique: 11699\n",
            "\n",
            "Finished A-Z index scraping. Found 11699 unique manager links.\n",
            "\n",
            "--- Manager Link Scraping Summary ---\n",
            "Total unique manager links found via A-Z Index: 11699\n",
            "\n",
            "Counts per Index Page:\n",
            "  Index '0' (/managers/0): 72 managers\n",
            "  Index 'A' (/managers/a): 1000 managers\n",
            "  Index 'B' (/managers/b): 777 managers\n",
            "  Index 'C' (/managers/c): 1139 managers\n",
            "  Index 'D' (/managers/d): 346 managers\n",
            "  Index 'E' (/managers/e): 369 managers\n",
            "  Index 'F' (/managers/f): 579 managers\n",
            "  Index 'G' (/managers/g): 484 managers\n",
            "  Index 'H' (/managers/h): 458 managers\n",
            "  Index 'I' (/managers/i): 315 managers\n",
            "  Index 'J' (/managers/j): 170 managers\n",
            "  Index 'K' (/managers/k): 309 managers\n",
            "  Index 'L' (/managers/l): 444 managers\n",
            "  Index 'M' (/managers/m): 735 managers\n",
            "  Index 'N' (/managers/n): 363 managers\n",
            "  Index 'O' (/managers/o): 250 managers\n",
            "  Index 'P' (/managers/p): 718 managers\n",
            "  Index 'Q' (/managers/q): 73 managers\n",
            "  Index 'R' (/managers/r): 477 managers\n",
            "  Index 'S' (/managers/s): 1172 managers\n",
            "  Index 'T' (/managers/t): 561 managers\n",
            "  Index 'U' (/managers/u): 97 managers\n",
            "  Index 'V' (/managers/v): 254 managers\n",
            "  Index 'W' (/managers/w): 442 managers\n",
            "  Index 'X' (/managers/x): 16 managers\n",
            "  Index 'Y' (/managers/y): 42 managers\n",
            "  Index 'Z' (/managers/z): 37 managers\n",
            "Stage 1 Duration: 7.94 seconds\n",
            "\n",
            "*** LIMITING MANAGERS TO 100 FOR TESTING/ASSESSMENT ***\n",
            "\n",
            "\n",
            "======================================================================\n",
            " Stage 2: Scraping Quarter Links for Each Selected Manager (Parallel)\n",
            "======================================================================\n",
            "  Processed quarter links for manager 50/100...\n",
            "  Processed quarter links for manager 100/100...\n",
            "\n",
            "Finished getting quarter links stage for 100 managers.\n",
            "Stage 2 Duration: 6.48 seconds\n",
            "\n",
            "======================================================================\n",
            " Stage 3: Scraping Filing Data for Each Manager/Quarter (Parallel)\n",
            "======================================================================\n",
            "Preparing filing data scrape tasks...\n",
            "Prepared 3284 total filing URLs to scrape in parallel.\n",
            "  Processed filing URL 100/3284...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:AJAX 'data' key missing or not list for https://13f.info/data/13f/000095012320011260.\n",
            "WARNING:root:AJAX did not yield DataFrame for https://13f.info/13f/000095012320011260-commonwealth-bank-of-australia-q3-2020.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Processed filing URL 200/3284...\n",
            "  Processed filing URL 300/3284...\n",
            "  Processed filing URL 400/3284...\n",
            "  Processed filing URL 500/3284...\n",
            "  Processed filing URL 600/3284...\n",
            "  Processed filing URL 700/3284...\n",
            "  Processed filing URL 800/3284...\n",
            "  Processed filing URL 900/3284...\n",
            "  Processed filing URL 1000/3284...\n",
            "  Processed filing URL 1100/3284...\n",
            "  Processed filing URL 1200/3284...\n",
            "  Processed filing URL 1300/3284...\n",
            "  Processed filing URL 1400/3284...\n",
            "  Processed filing URL 1500/3284...\n",
            "  Processed filing URL 1600/3284...\n",
            "  Processed filing URL 1700/3284...\n",
            "  Processed filing URL 1800/3284...\n",
            "  Processed filing URL 1900/3284...\n",
            "  Processed filing URL 2000/3284...\n",
            "  Processed filing URL 2100/3284...\n",
            "  Processed filing URL 2200/3284...\n",
            "  Processed filing URL 2300/3284...\n",
            "  Processed filing URL 2400/3284...\n",
            "  Processed filing URL 2500/3284...\n",
            "  Processed filing URL 2600/3284...\n",
            "  Processed filing URL 2700/3284...\n",
            "  Processed filing URL 2800/3284...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Retrying request for https://13f.info/data/13f/000156761920003877 due to HTTPError: 500 Server Error: Internal Server Error for url: https://13f.info/data/13f/000156761920003877 (Attempt 1/4). Waiting 1.00s...\n",
            "WARNING:root:Retrying request for https://13f.info/data/13f/000156761919021416 due to HTTPError: 500 Server Error: Internal Server Error for url: https://13f.info/data/13f/000156761919021416 (Attempt 1/4). Waiting 1.00s...\n",
            "WARNING:root:Retrying request for https://13f.info/data/13f/000156761919016888 due to HTTPError: 500 Server Error: Internal Server Error for url: https://13f.info/data/13f/000156761919016888 (Attempt 1/4). Waiting 1.00s...\n",
            "WARNING:root:Retrying request for https://13f.info/data/13f/000156761919010988 due to HTTPError: 500 Server Error: Internal Server Error for url: https://13f.info/data/13f/000156761919010988 (Attempt 1/4). Waiting 1.00s...\n",
            "WARNING:root:Retrying request for https://13f.info/data/13f/000156761919004235 due to HTTPError: 500 Server Error: Internal Server Error for url: https://13f.info/data/13f/000156761919004235 (Attempt 1/4). Waiting 1.00s...\n",
            "WARNING:root:Retrying request for https://13f.info/data/13f/000156761918006115 due to HTTPError: 500 Server Error: Internal Server Error for url: https://13f.info/data/13f/000156761918006115 (Attempt 1/4). Waiting 1.00s...\n",
            "WARNING:root:Retrying request for https://13f.info/data/13f/000156761918001119 due to HTTPError: 500 Server Error: Internal Server Error for url: https://13f.info/data/13f/000156761918001119 (Attempt 1/4). Waiting 1.00s...\n",
            "WARNING:root:Retrying request for https://13f.info/data/13f/000114036118023594 due to HTTPError: 500 Server Error: Internal Server Error for url: https://13f.info/data/13f/000114036118023594 (Attempt 1/4). Waiting 1.00s...\n",
            "WARNING:root:Retrying request for https://13f.info/data/13f/000114036118008283 due to HTTPError: 500 Server Error: Internal Server Error for url: https://13f.info/data/13f/000114036118008283 (Attempt 1/4). Waiting 1.00s...\n",
            "WARNING:root:Retrying request for https://13f.info/data/13f/000114036117042540 due to HTTPError: 500 Server Error: Internal Server Error for url: https://13f.info/data/13f/000114036117042540 (Attempt 1/4). Waiting 1.00s...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Processed filing URL 2900/3284...\n",
            "  Processed filing URL 3000/3284...\n",
            "  Processed filing URL 3100/3284...\n",
            "  Processed filing URL 3200/3284...\n",
            "  Processed filing URL 3284/3284...\n",
            "\n",
            "--- Finished Scraping All Filing Data ---\n",
            "Stage 3 Duration: 4083.96 seconds\n",
            "\n",
            "======================================================================\n",
            " Stage 4: Consolidating and Processing All Scraped Data\n",
            "======================================================================\n",
            "Combining 3283 scraped tables...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-d26ee5707bae>:616: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_df = pd.concat(all_scraped_dataframes, ignore_index=True, sort=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined DataFrame shape: (3194491, 13).\n",
            "\n",
            "Processing combined data...\n",
            "Filtered for 'COM' stock. Rows remaining: 1329679.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Non-numeric shares coerced to NaN.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating quarterly changes grouped by fund and 'sym'...\n",
            "Quarterly change calculations complete.\n",
            "\n",
            "Formatting final columns for output...\n",
            "Final DataFrame columns: ['fund_name', 'filing_date', 'quarter', 'stock symbol', 'cl', 'value_usd_000', 'shares', 'change', 'pct_change', 'inferred_transaction_type']\n",
            "\n",
            "Stage 4 Duration: 70.25 seconds\n",
            "\n",
            "======================================================================\n",
            " Stage 5: Final Output\n",
            "======================================================================\n",
            "\n",
            "--- First 5 rows of the final DataFrame (Shape: (1299736, 10)) ---\n",
            "                      fund_name filing_date               quarter stock symbol   cl  value_usd_000    shares  change  pct_change inferred_transaction_type\n",
            "2691865  300 NORTH CAPITAL, LLC   2/14/2014  Q4 2013 13F Holdings          ABC  COM           6016   85598.0       0    0.000000                      Hold\n",
            "2691761  300 NORTH CAPITAL, LLC   5/12/2014  Q1 2014 13F Holdings          ABC  COM           6035   92000.0    6402    0.074791                       Buy\n",
            "2691646  300 NORTH CAPITAL, LLC    8/5/2014  Q2 2014 13F Holdings          ABC  COM           7059   97146.0    5146    0.055935                       Buy\n",
            "2691523  300 NORTH CAPITAL, LLC  11/14/2014  Q3 2014 13F Holdings          ABC  COM           8190  105954.0    8808    0.090668                       Buy\n",
            "2691412  300 NORTH CAPITAL, LLC   2/11/2015  Q4 2014 13F Holdings          ABC  COM           8387   93034.0  -12920   -0.121940                      Sell\n",
            "\n",
            "--- Saving final DataFrame to 'Fund Manager Shares Analysis.csv' ---\n",
            "Successfully saved data (1299736 rows) to 'Fund Manager Shares Analysis.csv'\n",
            "\n",
            "Total Script Runtime: 4183.10 seconds (69.72 minutes)\n",
            "\n",
            "--- Script execution finished ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code for Full-Scale  13F Filings Scraper (All Managers, No Caps)**"
      ],
      "metadata": {
        "id": "COfc-4pfeLl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "#                            Imports and Setup\n",
        "# ==============================================================================\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "from urllib.parse import urljoin\n",
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Optional, Tuple, Any, Set\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed # For Parallelism\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, retry_if_exception # For Backoff\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)-8s - %(threadName)-10s - %(message)s', # Added threadName\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "# Suppress overly verbose logs from underlying libraries\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
        "\n",
        "# --- Constants ---\n",
        "BASE_URL: str = \"https://13f.info\"\n",
        "DEFAULT_USER_AGENT: str = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "CSV_FILENAME: str = \"Fund Manager Shares Analysis.csv\"\n",
        "\n",
        "# Define the exact columns desired in the final output, in order\n",
        "TARGET_FINAL_COLUMNS: List[str] = [\n",
        "    'fund_name', 'filing_date', 'quarter', 'stock symbol', 'cl',\n",
        "    'value_usd_000', 'shares', 'change', 'pct_change', 'inferred_transaction_type'\n",
        "]\n",
        "\n",
        "# Parallelism Configuration\n",
        "MAX_WORKERS: int = 15 # Adjust based on your system/network and risk tolerance (start lower, e.g., 10)\n",
        "# Backoff Configuration for Tenacity\n",
        "MAX_RETRIES: int = 4 # Max attempts per request (1 initial + 3 retries)\n",
        "INITIAL_BACKOFF_DELAY: float = 1.0 # Seconds for first retry delay base\n",
        "MAX_BACKOFF_DELAY: float = 10.0 # Max seconds to wait between retries\n",
        "\n",
        "logging.info(\"Script started. Libraries imported, logging and constants configured.\")\n",
        "logging.info(f\"Parallelism enabled with MAX_WORKERS={MAX_WORKERS}\")\n",
        "logging.info(f\"Exponential backoff configured: MAX_RETRIES={MAX_RETRIES}, INITIAL_DELAY={INITIAL_BACKOFF_DELAY}s, MAX_DELAY={MAX_BACKOFF_DELAY}s\")\n",
        "\n",
        "# ==============================================================================\n",
        "#              Request Function with Exponential Backoff (using Tenacity)\n",
        "# ==============================================================================\n",
        "\n",
        "# Define what network/connection exceptions tenacity should retry on\n",
        "RETRYABLE_NETWORK_EXCEPTIONS = (\n",
        "    requests.exceptions.Timeout,\n",
        "    requests.exceptions.ConnectionError,\n",
        "    requests.exceptions.ChunkedEncodingError # Can happen on intermittent network issues\n",
        ")\n",
        "\n",
        "# Define specific HTTP status codes to retry on (server errors and rate limiting)\n",
        "RETRYABLE_STATUS_CODES = {429, 500, 502, 503, 504}\n",
        "\n",
        "# Custom retry condition check for HTTP errors\n",
        "def _should_retry_http_error(exception: BaseException) -> bool:\n",
        "    \"\"\"Return True if the exception is an HTTPError with a retryable status code.\"\"\"\n",
        "    return (\n",
        "        isinstance(exception, requests.exceptions.HTTPError) and\n",
        "        hasattr(exception, 'response') and # Ensure response attribute exists\n",
        "        exception.response is not None and\n",
        "        exception.response.status_code in RETRYABLE_STATUS_CODES\n",
        "    )\n",
        "\n",
        "# Define the retry decorator\n",
        "retry_config = retry(\n",
        "    stop=stop_after_attempt(MAX_RETRIES),\n",
        "    wait=wait_exponential(multiplier=1, min=INITIAL_BACKOFF_DELAY, max=MAX_BACKOFF_DELAY),\n",
        "    # Retry on specific network exceptions OR specific HTTP error codes\n",
        "    retry=(retry_if_exception_type(RETRYABLE_NETWORK_EXCEPTIONS) | retry_if_exception(_should_retry_http_error)),\n",
        "    # Log before sleeping on retry\n",
        "    before_sleep=lambda retry_state: logging.warning(\n",
        "        f\"Retrying request for {retry_state.args[0]} \"\n",
        "        f\"due to {retry_state.outcome.exception().__class__.__name__}: {retry_state.outcome.exception()} \"\n",
        "        f\"(Attempt {retry_state.attempt_number}/{MAX_RETRIES}). Waiting {retry_state.next_action.sleep:.2f}s...\"\n",
        "    )\n",
        ")\n",
        "\n",
        "@retry_config\n",
        "def make_request_with_retries(url: str, headers: Dict[str, str], params: Optional[Dict[str, Any]] = None, timeout: int = 45) -> requests.Response:\n",
        "    \"\"\"\n",
        "    Makes a GET request using requests library with exponential backoff for specific errors,\n",
        "    leveraging the tenacity library.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL to request.\n",
        "        headers (Dict[str, str]): Request headers.\n",
        "        params (Optional[Dict[str, Any]]): URL parameters.\n",
        "        timeout (int): Request timeout in seconds.\n",
        "\n",
        "    Returns:\n",
        "        requests.Response: The response object if successful within retry limits.\n",
        "\n",
        "    Raises:\n",
        "        requests.exceptions.RequestException: If the request fails after all retries (or for non-retryable HTTP errors).\n",
        "        Exception: For other unexpected errors during the request phase.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, params=params, timeout=timeout)\n",
        "        response.raise_for_status() # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "        return response # Success!\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        # Log the final non-retryable HTTP error before raising\n",
        "        if not _should_retry_http_error(http_err):\n",
        "            logging.error(f\"Request to {url} failed permanently with status {http_err.response.status_code}. Error: {http_err}\")\n",
        "        raise # Re-raise the exception (tenacity decides based on should_retry)\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        # Log other request exceptions before raising/retrying\n",
        "        logging.error(f\"Request to {url} failed with connection/timeout error: {req_err}\")\n",
        "        raise # Re-raise the exception for tenacity\n",
        "    except Exception as e:\n",
        "         logging.error(f\"Unexpected error during request to {url}: {e}\", exc_info=True)\n",
        "         raise # Re-raise unexpected errors\n",
        "\n",
        "# ==============================================================================\n",
        "#                        Scraping Function Definitions\n",
        "# ==============================================================================\n",
        "# --- Helper Functions ---\n",
        "def _clean_header(header_text: str) -> str:\n",
        "    \"\"\"Helper function to consistently clean table header text.\"\"\"\n",
        "    if not isinstance(header_text, str): return \"\"\n",
        "    text = header_text.lower().strip()\n",
        "    text = text.replace(' ', '_')\n",
        "    text = text.replace('($000)', 'usd_000')\n",
        "    text = text.replace('%', 'pct')\n",
        "    text = re.sub(r'[^\\w_]+', '', text) # Keep underscores\n",
        "    return text\n",
        "\n",
        "def _extract_headers(table_tag: BeautifulSoup) -> List[str]:\n",
        "    \"\"\"Robustly extracts and cleans header text from a table tag.\"\"\"\n",
        "    header_cells = []\n",
        "    thead = table_tag.find('thead')\n",
        "    if thead:\n",
        "        header_row = thead.find('tr')\n",
        "        if header_row: header_cells = header_row.find_all(['th', 'td'])\n",
        "        if not header_cells: header_cells = thead.find_all('th')\n",
        "    if not header_cells:\n",
        "        first_row = table_tag.find('tr')\n",
        "        if first_row: header_cells = first_row.find_all(['th', 'td'])\n",
        "    if not header_cells:\n",
        "        logging.warning(\"Could not find any header cells (th/td) using multiple strategies.\")\n",
        "        return []\n",
        "    headers = [_clean_header(h.get_text()) for h in header_cells]\n",
        "    headers = [h for h in headers if h] # Filter empty strings\n",
        "    # logging.info(f\"Extracted {len(headers)} headers: {headers}\") # Can be noisy\n",
        "    return headers\n",
        "\n",
        "# --- Fallback BS4 Parsing ---\n",
        "def parse_tables_directly_bs4(soup_or_table_tag: Any, url: str, single_table: bool = False) -> List[pd.DataFrame]:\n",
        "    \"\"\"Parses table(s) directly from HTML using BeautifulSoup. Used as a fallback.\"\"\"\n",
        "    dataframes = []\n",
        "    potential_tables = []\n",
        "    if single_table:\n",
        "        potential_tables = [soup_or_table_tag] if soup_or_table_tag else []\n",
        "        mode = \"single table\"\n",
        "    else:\n",
        "        potential_tables = soup_or_table_tag.find_all('table', {'class': re.compile(r'\\btable\\b')}) if soup_or_table_tag else []\n",
        "        mode = \"all tables\"\n",
        "    # logging.info(f\"[Direct BS4 - {mode}] Parsing {len(potential_tables)} potential table(s) on {url}\")\n",
        "    if not potential_tables: return []\n",
        "\n",
        "    for i, table_tag in enumerate(potential_tables):\n",
        "        # logging.debug(f\"[Direct BS4] Processing table {i+1}\")\n",
        "        headers_list = _extract_headers(table_tag)\n",
        "        if not headers_list: continue\n",
        "        tbody = table_tag.find('tbody')\n",
        "        rows_in_body = tbody.find_all('tr') if tbody else (table_tag.find_all('tr')[1:] if len(table_tag.find_all('tr')) > 1 else [])\n",
        "        if not rows_in_body: continue\n",
        "\n",
        "        data_rows = []\n",
        "        expected_cols = len(headers_list)\n",
        "        column_mismatch_logged = False\n",
        "        for row_idx, row in enumerate(rows_in_body):\n",
        "            cells = row.find_all('td')\n",
        "            actual_cols = len(cells)\n",
        "            if actual_cols == expected_cols:\n",
        "                row_data = [c.get_text(strip=True) if c else None for c in cells]\n",
        "                data_rows.append(row_data)\n",
        "            else:\n",
        "                 if not column_mismatch_logged:\n",
        "                    logging.warning(f\"[Direct BS4] Table {i+1} on {url}: Row {row_idx} has {actual_cols} cells, expected {expected_cols}. Skipping mismatched rows for this table.\")\n",
        "                    column_mismatch_logged = True\n",
        "        if data_rows:\n",
        "            try:\n",
        "                df = pd.DataFrame(data_rows, columns=headers_list)\n",
        "                df['SourceURL'] = url\n",
        "                df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "                dataframes.append(df)\n",
        "                # logging.info(f\"[Direct BS4] Successfully created DataFrame (Shape: {df.shape}) from table {i+1} on {url}\")\n",
        "            except Exception as e:\n",
        "                 logging.error(f\"[Direct BS4] Error creating DataFrame for table {i+1} on {url}: {e}\", exc_info=True)\n",
        "    return dataframes\n",
        "\n",
        "# --- Primary Scraping Functions (using make_request_with_retries) ---\n",
        "\n",
        "def scrape_az_index_links(managers_index_page_url: str) -> List[str]:\n",
        "    \"\"\"Scrapes the A-Z (and 0-9) index links from the main managers listing page.\"\"\"\n",
        "    logging.info(f\"Scraping A-Z index links from: {managers_index_page_url}\")\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT}\n",
        "    index_links = set()\n",
        "    try:\n",
        "        response = make_request_with_retries(managers_index_page_url, headers=headers, timeout=30)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        index_container = soup.find('div', class_='mb-12')\n",
        "        links_in_container = index_container.find_all('a', href=True) if index_container else soup.select('ul > li > a[href]') # Fallback selector\n",
        "\n",
        "        for link_tag in links_in_container:\n",
        "            href = link_tag.get('href')\n",
        "            if isinstance(href, str) and href.startswith('/managers/') and len(href.split('/')[-1]) == 1:\n",
        "                index_links.add(href)\n",
        "\n",
        "        if not index_links:\n",
        "             logging.warning(f\"No valid A-Z index links extracted from {managers_index_page_url}\")\n",
        "             return []\n",
        "\n",
        "        logging.info(f\"Found {len(index_links)} A-Z index links: {sorted(list(index_links))}\")\n",
        "        return sorted(list(index_links))\n",
        "\n",
        "    except Exception as e: # Catch errors after retries from make_request or parsing errors\n",
        "        logging.error(f\"Failed to scrape or parse A-Z index page {managers_index_page_url} after retries. Error: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def scrape_managers_from_letter_page(letter_page_relative_url: str, base_url: str) -> Set[str]:\n",
        "    \"\"\"Scrapes all actual manager links from a specific A-Z index page.\"\"\"\n",
        "    full_url = urljoin(base_url, letter_page_relative_url)\n",
        "    # logging.info(f\"Scraping manager links from letter page: {full_url}\") # Noisy\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT}\n",
        "    manager_links_on_page = set()\n",
        "    try:\n",
        "        response = make_request_with_retries(full_url, headers=headers, timeout=45)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        all_links_on_page = soup.find_all('a', href=True)\n",
        "        for link_tag in all_links_on_page:\n",
        "            href = link_tag.get('href')\n",
        "            if isinstance(href, str) and href.startswith('/manager/'):\n",
        "                path_parts = href.strip('/').split('/')\n",
        "                if len(path_parts) == 2 and path_parts[0] == 'manager' and len(path_parts[1]) > 1:\n",
        "                    manager_links_on_page.add(href)\n",
        "        # logging.info(f\"Found {len(manager_links_on_page)} manager links on {full_url}\") # Can be noisy\n",
        "        return manager_links_on_page\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to scrape or parse letter page {full_url} after retries. Error: {e}\")\n",
        "        return set()\n",
        "\n",
        "\n",
        "def get_all_manager_links_via_az_index(base_url: str, executor: ThreadPoolExecutor) -> Tuple[List[str], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Orchestrates scraping manager links from A-Z index pages in parallel.\n",
        "    \"\"\"\n",
        "    all_unique_manager_links: Set[str] = set()\n",
        "    manager_counts_per_index_page: Dict[str, int] = {}\n",
        "    managers_main_page_url = urljoin(base_url, \"/managers\")\n",
        "\n",
        "    logging.info(\"Starting A-Z index link scraping...\")\n",
        "    az_index_links = scrape_az_index_links(managers_main_page_url)\n",
        "    if not az_index_links:\n",
        "        logging.critical(\"Could not retrieve A-Z index links. Cannot proceed.\")\n",
        "        return [], {}\n",
        "\n",
        "    total_az_pages = len(az_index_links)\n",
        "    logging.info(f\"Found {total_az_pages} A-Z index pages to process in parallel.\")\n",
        "    print(f\"Found {total_az_pages} A-Z index pages. Processing in parallel...\")\n",
        "\n",
        "    # Submit tasks to the executor\n",
        "    future_to_url = {executor.submit(scrape_managers_from_letter_page, url, base_url): url for url in az_index_links}\n",
        "    processed_count = 0\n",
        "\n",
        "    for future in as_completed(future_to_url):\n",
        "        rel_url = future_to_url[future]\n",
        "        index_char = rel_url.split('/')[-1].upper()\n",
        "        processed_count += 1\n",
        "        try:\n",
        "            manager_links_set = future.result() # Get the set of links from the completed task\n",
        "            count = len(manager_links_set)\n",
        "            manager_counts_per_index_page[rel_url] = count\n",
        "            all_unique_manager_links.update(manager_links_set)\n",
        "            logging.info(f\"[{processed_count}/{total_az_pages}] Index '{index_char}' completed. Found {count} links. Total unique: {len(all_unique_manager_links)}\")\n",
        "            print(f\"  Processed index '{index_char}'. Found {count} links. Total unique: {len(all_unique_manager_links)}\")\n",
        "        except Exception as exc:\n",
        "            logging.error(f\"Index page {rel_url} generated an exception: {exc}\")\n",
        "            manager_counts_per_index_page[rel_url] = 0 # Record failure\n",
        "\n",
        "    final_unique_count = len(all_unique_manager_links)\n",
        "    logging.info(f\"Finished parallel A-Z index scraping. Total unique manager links found: {final_unique_count}\")\n",
        "    print(f\"\\nFinished A-Z index scraping. Found {final_unique_count} unique manager links.\")\n",
        "    return sorted(list(all_unique_manager_links)), manager_counts_per_index_page\n",
        "\n",
        "\n",
        "def scrape_quarter_links_task(rel_link: str, base_url: str) -> Tuple[str, List[str]]:\n",
        "    \"\"\"Task wrapper for scraping quarter links for use with executor.\"\"\"\n",
        "    links = scrape_quarter_links_from_manager(rel_link, base_url) # Uses make_request internally\n",
        "    return rel_link, links\n",
        "\n",
        "def scrape_quarter_links_from_manager(relative_manager_link: str, base_url: str) -> List[str]:\n",
        "    \"\"\"Scrapes quarterly filing URLs from a manager page.\"\"\"\n",
        "    full_url = urljoin(base_url, relative_manager_link)\n",
        "    # logging.info(f\"Scraping quarter links from: {full_url}\") # Noisy\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT}\n",
        "    quarter_links = []\n",
        "    try:\n",
        "        response = make_request_with_retries(full_url, headers=headers, timeout=30)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        table = soup.find('table', {'class': re.compile(r'\\btable\\b')})\n",
        "        if not table: return []\n",
        "        header_row = table.find('thead') or table.find('tr')\n",
        "        if not header_row: return []\n",
        "        headers_list = header_row.find_all(['th', 'td'])\n",
        "        quarter_col_index = -1\n",
        "        for i, header in enumerate(headers_list):\n",
        "            if header.get_text(strip=True).lower() == \"quarter\":\n",
        "                quarter_col_index = i; break\n",
        "        if quarter_col_index == -1: return []\n",
        "        tbody = table.find('tbody')\n",
        "        data_rows = tbody.find_all('tr') if tbody else (table.find_all('tr')[1:] if len(table.find_all('tr')) > 1 else [])\n",
        "        if not data_rows: return []\n",
        "        for row in data_rows:\n",
        "            cells = row.find_all('td')\n",
        "            if len(cells) > quarter_col_index:\n",
        "                link_tag = cells[quarter_col_index].find('a', href=True)\n",
        "                href_val = link_tag.get('href') if link_tag else None\n",
        "                if isinstance(href_val, str) and href_val.strip():\n",
        "                    quarter_links.append(urljoin(base_url, href_val))\n",
        "        # logging.info(f\"Found {len(quarter_links)} quarter links on {full_url}.\") # Noisy\n",
        "        return quarter_links\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed scrape_quarter_links_from_manager for {full_url}. Error: {e}\")\n",
        "        return [] # Return empty list on failure after retries\n",
        "\n",
        "# ---\n",
        "def scrape_fund_name(manager_page_url: str) -> str:\n",
        "    \"\"\"Scrapes the fund name from the manager's main page (using retries).\"\"\"\n",
        "    # logging.info(f\"Scraping fund name from: {manager_page_url}\") # Noisy\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT}\n",
        "    default_name = \"Unknown Fund Name\"\n",
        "    try:\n",
        "        response = make_request_with_retries(manager_page_url, headers=headers, timeout=25)\n",
        "        soup = BeautifulSoup(response.text, 'lxml')\n",
        "        h1_tag = soup.find('h1')\n",
        "        if h1_tag:\n",
        "            name = h1_tag.get_text(strip=True)\n",
        "            if ' CIK#' in name: name = name.split(' CIK#')[0].strip()\n",
        "            # logging.info(f\"Found fund name via H1: {name}\") # Noisy\n",
        "            return name if name else default_name\n",
        "        else: # Fallback to URL parsing\n",
        "            logging.warning(f\"H1 tag not found on {manager_page_url}. Attempting URL parsing.\")\n",
        "            try:\n",
        "                path_parts = manager_page_url.strip('/').split('/')\n",
        "                if len(path_parts) > 3 and path_parts[-2] == 'manager':\n",
        "                    url_name_part = path_parts[-1]\n",
        "                    first_dash_index = url_name_part.find('-')\n",
        "                    if first_dash_index != -1:\n",
        "                         parsed_name = url_name_part[first_dash_index:].strip('-').replace('-', ' ').title()\n",
        "                         if parsed_name: return parsed_name\n",
        "            except Exception: pass\n",
        "            return default_name\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed scrape_fund_name for {manager_page_url} after retries. Error: {e}\")\n",
        "        return default_name\n",
        "\n",
        "# ---\n",
        "def scrape_table_data_and_metadata(url: str) -> Dict[str, Any]:\n",
        "    \"\"\"Scrapes table data and metadata from a filing URL (using retries).\"\"\"\n",
        "    # logging.info(f\"Scraping table data & metadata from: {url}\") # Noisy\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT, 'Accept': 'application/json, text/javascript, */*; q=0.01', 'X-Requested-With': 'XMLHttpRequest'}\n",
        "    result: Dict[str, Any] = {'dataframes': [], 'filing_date': None, 'quarter': None}\n",
        "    html_content = None\n",
        "    soup = None\n",
        "    try:\n",
        "        response_page = make_request_with_retries(url, headers=headers, timeout=45)\n",
        "        html_content = response_page.text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to retrieve page {url} after retries. Error: {e}\")\n",
        "        return result # Cannot proceed\n",
        "\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "        # Extract Metadata\n",
        "        date_dt = soup.find('dt', string=lambda t: t and 'Date filed' in t.strip())\n",
        "        if date_dt and date_dt.find_next_sibling('dd'): result['filing_date'] = date_dt.find_next_sibling('dd').get_text(strip=True)\n",
        "        qtr_h = soup.find(['h1', 'h2', 'h3'], string=lambda t: t and ('13F Holdings' in t or 'Quarter' in t))\n",
        "        if qtr_h: result['quarter'] = qtr_h.get_text(strip=True)\n",
        "    except Exception as e: logging.error(f\"Error parsing metadata from {url}: {e}\")\n",
        "\n",
        "    target_table = soup.find('table', id='filingAggregated') if soup else None\n",
        "    if not target_table:\n",
        "        # logging.warning(f\"Target table not found. Parsing all tables on page: {url}\")\n",
        "        if soup: result['dataframes'] = parse_tables_directly_bs4(soup, url)\n",
        "        return result\n",
        "\n",
        "    headers_list = _extract_headers(target_table)\n",
        "    if not headers_list:\n",
        "        logging.error(f\"CRITICAL: Failed to extract HTML headers from target table on {url}.\")\n",
        "        result['dataframes'] = parse_tables_directly_bs4(target_table, url, single_table=True)\n",
        "        return result\n",
        "\n",
        "    data_url_path = target_table.get('data-url')\n",
        "    ajax_succeeded = False\n",
        "    if data_url_path:\n",
        "        ajax_url = urljoin(BASE_URL, data_url_path)\n",
        "        # logging.info(f\"AJAX endpoint detected. Attempting fetch from: {ajax_url}\") # Noisy\n",
        "        try:\n",
        "            response_ajax = make_request_with_retries(ajax_url, headers=headers, timeout=45)\n",
        "            ajax_data = response_ajax.json()\n",
        "            table_rows_data = ajax_data.get('data')\n",
        "            if table_rows_data and isinstance(table_rows_data, list):\n",
        "                expected_cols = len(headers_list)\n",
        "                cleaned_rows = []\n",
        "                for row in table_rows_data:\n",
        "                    if isinstance(row, list):\n",
        "                        actual_cols = len(row)\n",
        "                        if actual_cols >= expected_cols:\n",
        "                            processed_row = row[:expected_cols]\n",
        "                            cleaned_cells = [BeautifulSoup(str(c), 'lxml').get_text(strip=True) if isinstance(c, str) else c for c in processed_row]\n",
        "                            cleaned_rows.append(cleaned_cells)\n",
        "                if cleaned_rows:\n",
        "                    df = pd.DataFrame(cleaned_rows, columns=headers_list)\n",
        "                    df['SourceURL'] = url\n",
        "                    result['dataframes'].append(df)\n",
        "                    ajax_succeeded = True\n",
        "                    # logging.info(f\"Created DataFrame (Shape: {df.shape}) from AJAX.\") # Noisy\n",
        "            else: logging.warning(f\"AJAX 'data' key missing or not list for {ajax_url}.\")\n",
        "        except Exception as e: logging.error(f\"AJAX request/processing failed for {ajax_url} after retries. Error: {e}\")\n",
        "        if ajax_succeeded: return result\n",
        "        else: logging.warning(f\"AJAX did not yield DataFrame for {url}.\")\n",
        "\n",
        "    # Fallback if no data-url or AJAX failed\n",
        "    # logging.info(f\"Falling back to direct HTML parsing for target table on: {url}\") # Noisy\n",
        "    result['dataframes'] = parse_tables_directly_bs4(target_table, url, single_table=True)\n",
        "    return result\n",
        "\n",
        "\n",
        "# --- Wrapper function for parallel data scraping task ---\n",
        "def scrape_filing_data_task(data_url: str, fund_name: str) -> List[pd.DataFrame]:\n",
        "    \"\"\"Scrapes data for one filing URL and adds fund_name.\"\"\"\n",
        "    scrape_result = scrape_table_data_and_metadata(data_url)\n",
        "    processed_dfs = []\n",
        "    for df in scrape_result.get('dataframes', []):\n",
        "        df['fund_name'] = fund_name\n",
        "        df['filing_date'] = scrape_result.get('filing_date')\n",
        "        df['quarter'] = scrape_result.get('quarter')\n",
        "        processed_dfs.append(df)\n",
        "    return processed_dfs\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#                        Execution Stage 1: Get All Manager Links (A-Z Parallel)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 1: Scraping All Manager Links via A-Z Index (Parallel)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time_stage1 = time.time()\n",
        "all_manager_links = []\n",
        "manager_counts = {}\n",
        "\n",
        "# Use ThreadPoolExecutor for parallel execution\n",
        "# The 'with' statement ensures threads are cleaned up properly\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS, thread_name_prefix='AZScrape') as executor:\n",
        "    all_manager_links, manager_counts = get_all_manager_links_via_az_index(BASE_URL, executor)\n",
        "\n",
        "end_time_stage1 = time.time()\n",
        "print(f\"\\n--- Manager Link Scraping Summary ---\")\n",
        "print(f\"Total unique manager links found via A-Z Index: {len(all_manager_links)}\")\n",
        "if manager_counts:\n",
        "    print(\"\\nCounts per Index Page:\")\n",
        "    # Sort counts by index page URL for consistent output\n",
        "    for page_url, count in sorted(manager_counts.items()):\n",
        "        index_char = page_url.split('/')[-1].upper()\n",
        "        print(f\"  Index '{index_char}' ({page_url}): {count} managers\")\n",
        "print(f\"Stage 1 Duration: {end_time_stage1 - start_time_stage1:.2f} seconds\")\n",
        "\n",
        "if not all_manager_links:\n",
        "    print(\"\\nCRITICAL: No manager links were found. Cannot proceed.\")\n",
        "    exit()\n",
        "\n",
        "# --- Remove the Test Limit ---\n",
        "# The block limiting the managers is now commented out / removed\n",
        "# Optional: Limit managers for testing (apply AFTER getting all links)\n",
        "# test_limit_stage1 = 100 # Set a reasonable limit for testing\n",
        "# if len(all_manager_links) > test_limit_stage1:\n",
        "#     logging.info(f\"TESTING: Limiting run to first {test_limit_stage1} managers found.\")\n",
        "#     print(f\"\\n*** LIMITING MANAGERS TO {test_limit_stage1} FOR TESTING/ASSESSMENT ***\\n\")\n",
        "#     manager_links_relative = all_manager_links[:test_limit_stage1]\n",
        "# else:\n",
        "#      manager_links_relative = all_manager_links # Process all if less than limit\n",
        "\n",
        "# Process ALL manager links found\n",
        "manager_links_relative = all_manager_links\n",
        "print(f\"\\nProcessing ALL {len(manager_links_relative)} unique managers found...\")\n",
        "\n",
        "time.sleep(random.uniform(0.5, 1.0)) # Small delay\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#                 Execution Stage 2: Get Quarter Links (Parallel)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 2: Scraping Quarter Links for Each Selected Manager (Parallel)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time_stage2 = time.time()\n",
        "manager_quarter_links: Dict[str, List[str]] = {}\n",
        "num_managers_to_process = len(manager_links_relative) # This now uses the full list\n",
        "processed_count_stage2 = 0\n",
        "print(f\"Fetching quarter links for {num_managers_to_process} managers...\")\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS, thread_name_prefix='QuarterLinkScrape') as executor:\n",
        "    future_to_rel_link = {executor.submit(scrape_quarter_links_task, rel_link, BASE_URL): rel_link for rel_link in manager_links_relative}\n",
        "\n",
        "    for future in as_completed(future_to_rel_link):\n",
        "        rel_link = future_to_rel_link[future]\n",
        "        processed_count_stage2 += 1\n",
        "        try:\n",
        "            _rel_link_result, links = future.result() # Unpack tuple\n",
        "            manager_quarter_links[rel_link] = links\n",
        "            # Log progress periodically or based on count\n",
        "            if processed_count_stage2 % 100 == 0 or processed_count_stage2 == num_managers_to_process: # Log every 100\n",
        "                 logging.info(f\"Quarter links progress: {processed_count_stage2}/{num_managers_to_process} managers processed.\")\n",
        "                 print(f\"  Processed quarter links for manager {processed_count_stage2}/{num_managers_to_process}...\")\n",
        "            if not links:\n",
        "                 logging.warning(f\"No quarter links found or error occurred for {rel_link}\")\n",
        "        except Exception as exc:\n",
        "            logging.error(f\"Getting quarter links for {rel_link} generated an exception: {exc}\")\n",
        "            manager_quarter_links[rel_link] = [] # Ensure entry exists even on failure\n",
        "\n",
        "end_time_stage2 = time.time()\n",
        "print(f\"\\nFinished getting quarter links stage for {len(manager_quarter_links)} managers.\")\n",
        "print(f\"Stage 2 Duration: {end_time_stage2 - start_time_stage2:.2f} seconds\")\n",
        "\n",
        "# ==============================================================================\n",
        "#                 Execution Stage 3: Scrape Filing Data (Parallel)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 3: Scraping Filing Data for Each Manager/Quarter (Parallel)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time_stage3 = time.time()\n",
        "all_scraped_dataframes: List[pd.DataFrame] = []\n",
        "fund_name_map: Dict[str, str] = {} # Cache fund names\n",
        "\n",
        "# Prepare list of tasks: (data_url, fund_name)\n",
        "tasks_to_submit = []\n",
        "print(\"Preparing filing data scrape tasks...\")\n",
        "# Use manager_links_relative (which contains ALL managers now) to iterate\n",
        "for i, manager_link in enumerate(manager_links_relative):\n",
        "    # Get or Cache Fund Name - Fetch sequentially before parallel data scrape\n",
        "    # This avoids potential race conditions if multiple threads tried to scrape the same name\n",
        "    if manager_link not in fund_name_map:\n",
        "        manager_page_url = urljoin(BASE_URL, manager_link)\n",
        "        fund_name_map[manager_link] = scrape_fund_name(manager_page_url)\n",
        "        logging.info(f\"Fetched name for {manager_link}: {fund_name_map[manager_link]}\")\n",
        "        if (i + 1) % 50 == 0: # Print progress for name scraping\n",
        "            print(f\"  Fetched names for {i+1}/{num_managers_to_process} managers...\")\n",
        "        time.sleep(random.uniform(0.1, 0.5)) # Small delay after name scrape\n",
        "\n",
        "    current_fund_name = fund_name_map[manager_link]\n",
        "    quarter_links = manager_quarter_links.get(manager_link, []) # Get links fetched in Stage 2\n",
        "\n",
        "    # Determine URLs to scrape for this manager\n",
        "    urls_to_scrape = quarter_links if quarter_links else [urljoin(BASE_URL, manager_link)]\n",
        "    if not quarter_links:\n",
        "        logging.warning(f\"No specific quarter links for {manager_link}, will attempt manager page scrape.\")\n",
        "\n",
        "    for data_url in urls_to_scrape:\n",
        "        tasks_to_submit.append((data_url, current_fund_name))\n",
        "\n",
        "total_filing_urls = len(tasks_to_submit)\n",
        "print(f\"\\nPrepared {total_filing_urls} total filing URLs to scrape in parallel.\")\n",
        "logging.info(f\"Submitting {total_filing_urls} filing data scraping tasks to executor.\")\n",
        "\n",
        "processed_count_stage3 = 0\n",
        "# Use ThreadPoolExecutor for parallel data scraping\n",
        "with ThreadPoolExecutor(max_workers=MAX_WORKERS, thread_name_prefix='FilingDataScrape') as executor:\n",
        "    future_to_task_info = {executor.submit(scrape_filing_data_task, url, name): (url, name) for url, name in tasks_to_submit}\n",
        "\n",
        "    for future in as_completed(future_to_task_info):\n",
        "        url, name = future_to_task_info[future]\n",
        "        processed_count_stage3 += 1\n",
        "        try:\n",
        "            list_of_dfs = future.result() # Result is a list of DFs for that URL\n",
        "            if list_of_dfs:\n",
        "                all_scraped_dataframes.extend(list_of_dfs)\n",
        "                # Log progress periodically\n",
        "                if processed_count_stage3 % 200 == 0 or processed_count_stage3 == total_filing_urls: # Log every 200 URLs\n",
        "                    logging.info(f\"Filing data progress: {processed_count_stage3}/{total_filing_urls} URLs processed. Total DFs collected: {len(all_scraped_dataframes)}\")\n",
        "                    print(f\"  Processed filing URL {processed_count_stage3}/{total_filing_urls}...\")\n",
        "            # else: # No dataframes found for this URL (already logged in scrape function)\n",
        "                # pass\n",
        "        except Exception as exc:\n",
        "            logging.error(f\"Scraping task for URL {url} (Fund: {name}) generated an exception: {exc}\")\n",
        "\n",
        "end_time_stage3 = time.time()\n",
        "print(\"\\n--- Finished Scraping All Filing Data ---\")\n",
        "print(f\"Collected data resulting in {len(all_scraped_dataframes)} total DataFrames (pre-concat).\")\n",
        "print(f\"Stage 3 Duration: {end_time_stage3 - start_time_stage3:.2f} seconds\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#               Execution Stage 4: Data Consolidation & Processing\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 4: Consolidating and Processing All Scraped Data\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "start_time_stage4 = time.time()\n",
        "\n",
        "if not all_scraped_dataframes:\n",
        "    logging.warning(\"No dataframes were scraped in Stage 3. Cannot proceed with processing.\")\n",
        "    final_processed_df = pd.DataFrame() # Ensure variable exists but is empty\n",
        "else:\n",
        "    # --- Combine all scraped dataframes ---\n",
        "    logging.info(f\"Combining {len(all_scraped_dataframes)} scraped dataframes...\")\n",
        "    print(f\"Combining {len(all_scraped_dataframes)} scraped tables...\")\n",
        "    try:\n",
        "        combined_df = pd.concat(all_scraped_dataframes, ignore_index=True, sort=False)\n",
        "        logging.info(f\"Initial combined DataFrame shape: {combined_df.shape}\")\n",
        "        print(f\"Combined DataFrame shape: {combined_df.shape}.\")\n",
        "        # It's crucial to work on a copy for significant processing\n",
        "        processed_df = combined_df.copy()\n",
        "        del combined_df, all_scraped_dataframes # Free up memory\n",
        "    except Exception as e:\n",
        "        logging.critical(f\"CRITICAL Error during DataFrame concatenation: {e}\", exc_info=True)\n",
        "        processed_df = pd.DataFrame() # Assign empty df on critical error\n",
        "\n",
        "# --- Process the combined dataframe (only if concatenation was successful) ---\n",
        "if not processed_df.empty:\n",
        "    logging.info(\"Starting data processing steps...\")\n",
        "    print(\"\\nProcessing combined data...\")\n",
        "\n",
        "    # --- Standardize Column Names ---\n",
        "    processed_df.columns = [_clean_header(col) for col in processed_df.columns]\n",
        "    logging.info(f\"Standardized column names: {processed_df.columns.tolist()}\")\n",
        "\n",
        "    # --- Filter for Common Stock ('COM') ---\n",
        "    if 'cl' in processed_df.columns:\n",
        "        original_rows = len(processed_df)\n",
        "        processed_df = processed_df[processed_df['cl'].astype(str).str.upper() == 'COM']\n",
        "        logging.info(f\"Filtered for 'cl' == 'COM'. Rows reduced from {original_rows} to {len(processed_df)}.\")\n",
        "        print(f\"Filtered for 'COM' stock. Rows remaining: {len(processed_df)}.\")\n",
        "    else: logging.warning(\"'cl' column not found.\")\n",
        "\n",
        "    # --- Data Type Conversion and Cleaning ---\n",
        "    logging.info(\"Cleaning and converting data types...\")\n",
        "    # Shares\n",
        "    if 'shares' in processed_df.columns:\n",
        "        processed_df['shares'] = processed_df['shares'].astype(str).str.replace(',', '', regex=False)\n",
        "        processed_df['shares_numeric'] = pd.to_numeric(processed_df['shares'], errors='coerce')\n",
        "        if processed_df['shares_numeric'].isna().any(): logging.warning(\"Non-numeric shares coerced to NaN.\")\n",
        "    else: processed_df['shares_numeric'] = np.nan\n",
        "    # Value\n",
        "    value_col = 'value_usd_000'\n",
        "    if value_col in processed_df.columns:\n",
        "         processed_df[value_col] = processed_df[value_col].astype(str).str.replace(',', '', regex=False)\n",
        "         processed_df[value_col] = pd.to_numeric(processed_df[value_col], errors='coerce')\n",
        "         if processed_df[value_col].isna().any(): logging.warning(f\"Non-numeric '{value_col}' coerced to NaN.\")\n",
        "    else: logging.warning(f\"Value column '{value_col}' not found.\")\n",
        "    # Quarter Period\n",
        "    if 'quarter' in processed_df.columns:\n",
        "        def parse_quarter_to_period(q_str: Optional[str]) -> Optional[pd.Period]:\n",
        "            if not isinstance(q_str, str): return pd.NaT\n",
        "            match = re.search(r'(Q[1-4])\\s*(\\d{4})', q_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                q_num, year = match.group(1).upper(), match.group(2)\n",
        "                try: return pd.Period(f\"{year}{q_num}\", freq='Q')\n",
        "                except ValueError: return pd.NaT\n",
        "            return pd.NaT\n",
        "        processed_df['quarter_period'] = processed_df['quarter'].apply(parse_quarter_to_period)\n",
        "        if processed_df['quarter_period'].isna().any(): logging.warning(\"Some quarters could not be parsed.\")\n",
        "    else: processed_df['quarter_period'] = pd.NaT\n",
        "\n",
        "    # --- Identify Stock Identifier ---\n",
        "    stock_id_col = 'sym' if 'sym' in processed_df.columns else ('stock_symbol' if 'stock_symbol' in processed_df.columns else None)\n",
        "    if not stock_id_col: logging.error(\"CRITICAL: No stock identifier found.\")\n",
        "\n",
        "    # --- Calculate Changes ---\n",
        "    can_calculate_changes = stock_id_col and ('shares_numeric' in processed_df.columns) and ('quarter_period' in processed_df.columns) and processed_df['quarter_period'].notna().any()\n",
        "    if can_calculate_changes:\n",
        "        logging.info(f\"Calculating changes grouped by 'fund_name', '{stock_id_col}'...\")\n",
        "        print(f\"Calculating quarterly changes grouped by fund and '{stock_id_col}'...\")\n",
        "        essential_cols = ['fund_name', stock_id_col, 'quarter_period', 'shares_numeric']\n",
        "        processed_df.dropna(subset=[col for col in essential_cols if col in processed_df.columns], inplace=True)\n",
        "        processed_df = processed_df.sort_values(by=['fund_name', stock_id_col, 'quarter_period'], ascending=True)\n",
        "        group_cols = ['fund_name', stock_id_col]\n",
        "        for col in ['change', 'pct_change', 'inferred_transaction_type']:\n",
        "             if col not in processed_df.columns: processed_df[col] = pd.NA\n",
        "        processed_df['change'] = processed_df.groupby(group_cols, observed=True)['shares_numeric'].diff().fillna(0)\n",
        "        pct_change_raw = processed_df.groupby(group_cols, observed=True)['shares_numeric'].pct_change()\n",
        "        processed_df['pct_change'] = pct_change_raw.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "        change_numeric = pd.to_numeric(processed_df['change'], errors='coerce')\n",
        "        conditions = [change_numeric < 0, change_numeric == 0, change_numeric > 0]\n",
        "        choices = ['Sell', 'Hold', 'Buy']\n",
        "        processed_df['inferred_transaction_type'] = np.select(conditions, choices, default='Unknown')\n",
        "        logging.info(\"Change calculations complete.\")\n",
        "        print(\"Quarterly change calculations complete.\")\n",
        "        try: processed_df['change'] = processed_df['change'].astype(pd.Int64Dtype())\n",
        "        except TypeError: pass\n",
        "    else:\n",
        "        logging.warning(\"Skipping quarterly change calculations.\")\n",
        "        print(\"Skipping quarterly change calculations.\")\n",
        "        for col in ['change', 'pct_change', 'inferred_transaction_type']:\n",
        "             if col not in processed_df.columns: processed_df[col] = pd.NA\n",
        "\n",
        "    # --- Final Column Formatting ---\n",
        "    logging.info(\"Formatting final columns...\")\n",
        "    print(\"\\nFormatting final columns for output...\")\n",
        "    if stock_id_col == 'sym' and 'sym' in processed_df.columns:\n",
        "        processed_df.rename(columns={'sym': 'stock symbol'}, inplace=True)\n",
        "        logging.info(\"Renamed 'sym' column to 'stock symbol'.\")\n",
        "    elif 'stock symbol' not in processed_df.columns and 'stock symbol' in TARGET_FINAL_COLUMNS:\n",
        "         logging.warning(\"Final target 'stock symbol' column not found. Creating with NaNs.\")\n",
        "         processed_df['stock symbol'] = np.nan\n",
        "\n",
        "    final_columns_existing = [col for col in TARGET_FINAL_COLUMNS if col in processed_df.columns]\n",
        "    missing_target_cols = [col for col in TARGET_FINAL_COLUMNS if col not in final_columns_existing]\n",
        "    if missing_target_cols: logging.warning(f\"Final target columns missing: {missing_target_cols}\")\n",
        "\n",
        "    final_processed_df = processed_df[final_columns_existing]\n",
        "    logging.info(f\"Final selected columns: {final_processed_df.columns.tolist()}\")\n",
        "    print(f\"Final DataFrame columns: {final_processed_df.columns.tolist()}\")\n",
        "    final_processed_df = final_processed_df.drop(columns=['shares_numeric', 'quarter_period'], errors='ignore')\n",
        "\n",
        "else: # combined_df was empty or processing failed\n",
        "    logging.warning(\"Combined DataFrame was empty or processing failed.\")\n",
        "    final_processed_df = pd.DataFrame()\n",
        "\n",
        "end_time_stage4 = time.time()\n",
        "print(f\"\\nStage 4 Duration: {end_time_stage4 - start_time_stage4:.2f} seconds\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#                        Execution Stage 5: Final Output\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 5: Final Output\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if not final_processed_df.empty:\n",
        "    # --- Print Head ---\n",
        "    print(f\"\\n--- First 5 rows of the final DataFrame (Shape: {final_processed_df.shape}) ---\")\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.width', 200)\n",
        "    pd.set_option('display.max_colwidth', 70)\n",
        "    print(final_processed_df.head().to_string())\n",
        "    pd.reset_option('display.max_colwidth')\n",
        "\n",
        "    # --- Save to CSV ---\n",
        "    print(f\"\\n--- Saving final DataFrame to '{CSV_FILENAME}' ---\")\n",
        "    try:\n",
        "        final_processed_df.to_csv(CSV_FILENAME, index=False, encoding='utf-8-sig')\n",
        "        print(f\"Successfully saved data ({len(final_processed_df)} rows) to '{CSV_FILENAME}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL ERROR: Failed to save the final DataFrame to CSV '{CSV_FILENAME}'. Error: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nFinal DataFrame is empty or was not created due to errors. No CSV file saved.\")\n",
        "\n",
        "# --- Calculate and Print Total Runtime ---\n",
        "total_end_time = time.time()\n",
        "# Need start time from the very beginning if defined, otherwise calculate from stages\n",
        "# Assuming start_time_stage1 exists\n",
        "if 'start_time_stage1' in locals():\n",
        "     total_runtime = total_end_time - start_time_stage1\n",
        "     print(f\"\\nTotal Script Runtime: {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)\")\n",
        "else:\n",
        "     # Sum stage durations if start_time_stage1 wasn't captured\n",
        "     total_runtime = (end_time_stage1 - start_time_stage1 if 'start_time_stage1' in locals() else 0) + \\\n",
        "                     (end_time_stage2 - start_time_stage2 if 'start_time_stage2' in locals() else 0) + \\\n",
        "                     (end_time_stage3 - start_time_stage3 if 'start_time_stage3' in locals() else 0) + \\\n",
        "                     (end_time_stage4 - start_time_stage4 if 'start_time_stage4' in locals() else 0)\n",
        "     if total_runtime > 0:\n",
        "          print(f\"\\nTotal Script Runtime (Sum of Stages): {total_runtime:.2f} seconds ({total_runtime/60:.2f} minutes)\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Script execution finished ---\")"
      ],
      "metadata": {
        "id": "AUI58ZKYeH5H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/U3zQX2iGmp8HHOy76D7t",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}